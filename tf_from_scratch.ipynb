{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the input embedding layer first where text is converted as tokens into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # in AIYUN paper, it 512\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)  # Acc to paper page 5\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Positional encoding and add it to Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # in this paper, it 512\n",
    "        self.seq_len = seq_len  # maximum length of the sequence (the length of a sentence for example)\n",
    "        self.dropout = nn.Dropout(p=dropout) # Droput to add noise and improve generalization (google for more details)\n",
    "        \n",
    "        # Initialise the position encoding matrix of shape (sequence length(seq_len) , d_model=512) \n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # create a vector of shape (seq_len, 1)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "        # Positional encoding formula as per page 6\n",
    "        # now, we will create the denominator of the positional encoding formulae\n",
    "        # since it is a bit long, we will break it into a few lines\n",
    "        # first, we need a vector containing multiples of 2 from 0 to d_model (here, 512)\n",
    "        # this line is because of the 2i term which is the power of 10000\n",
    "        # thus, this vector provides for the numbers we need for 2i\n",
    "        vector = torch.arange(0, d_model, 2, dtype=torch.float32)\n",
    "        # now, we raise 10,000 to the power of 2i/d_model\n",
    "        # denominator_original = torch.pow(10000, vector/d_model)\n",
    "        # this is the one used by Harvard Transformer article (exponential of log nullifies but helps in numerical stability)\n",
    "        denominator_harvard = torch.exp(vector * (-math.log(10000.0)/d_model))\n",
    "        \n",
    "        # even : apply sin and store it in even indices of pe {start from 0 and increment by 2 for index}\n",
    "        pe[:, 0::2] = torch.sin(position * denominator_harvard)\n",
    "        # odd : apply cos and store it in odd indices of pe {start from 1 and increment by 2 for index}\n",
    "        pe[:, 1::2] = torch.cos(position * denominator_harvard)\n",
    "        \n",
    "        # we need to add the batch dimension so that we can apply it to batches of sentences\n",
    "        pe = pe.unsqueeze(0)  # new shape: (1, seq_len, d_model)\n",
    "        # register the pe tensor as a buffer so that it can be saved along with the state of the model\n",
    "        # used for storing auxillary and non-trainable data , associated with nn.module {google for more information}\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we don't want to train the positional encoding, ie, we don't want to make it\n",
    "        # a learnable parameter, so we set its requires_grad to False\n",
    "        x = x + self.pe[:, :x.size(1)].requires_grad_(False)  # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy_obj = tensor([[[ 2.4086,  3.1091,  1.1259, -0.0000],\n",
      "         [ 1.8999, -0.8678, -0.0413, -0.7559],\n",
      "         [ 0.1965,  1.5407, -0.4656, -0.0000],\n",
      "         [-0.0000, -1.9368, -0.9236,  2.2025]]])\n",
      "Shape of dummy_obj =torch.Size([1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "def dummyfn2():\n",
    "    torch.manual_seed(42)\n",
    "    seq_len = 4\n",
    "    d_model = 4\n",
    "    dropout = 0.2\n",
    "    x = torch.randn(d_model, seq_len)\n",
    "    obj = PositionalEncoding(d_model, seq_len, dropout)\n",
    "    return obj(x)\n",
    "\n",
    "dummy_obj = dummyfn2()\n",
    "print(f\"dummy_obj = {dummy_obj}\")\n",
    "print(f\"Shape of dummy_obj ={dummy_obj.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Layer Normalisation Block ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # instead of simply doing self.alpha = torch.ones(1)\n",
    "        # we use nn.Parameter() so that when we call the state dict of the model\n",
    "        # we are able to see this alpha & be able to learn it.\n",
    "        # only using torch.ones(1) won't allow us to see this alpha\n",
    "        self.alpha = nn.Parameter(torch.ones(1))  # multiplied\n",
    "        self.bias = nn.Parameter(torch.zeros(1))  # added\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply mean after the batch dimension\n",
    "        # mean usually cancels the dimension to which it is applied,\n",
    "        # but we want to keep it hence keepdim=true is used here\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        # similarly for standard deviation\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha * ((x-mean)/(std**2 + self.eps)) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the FeedForward Block ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"Implements the FFN equation.\"\"\"\n",
    "    # See euation (2) on page 5\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)   # W1 and B1\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)   # W2 and B2\n",
    "        self.dropout = nn.Dropout(p=dropout)      # p = Dropout Rate\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is of the shape: (batch, seq_len, d_model)\n",
    "        linear1 is of the shape: (d_model, d_ff)\n",
    "        linear2 is of the shape: (d_ff, d_model)\n",
    "\n",
    "        On multiplying x with linear1, the shape of x becomes (batch, seq_len, d_ff)\n",
    "        On multiplying the new x with linear2, the shape of x changes back to the\n",
    "        original one, ie, (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the MultiHead attention module ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        \"\"\"Take in model size and number of heads.\"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # embedding vector size\n",
    "        self.h = h   # number of heads\n",
    "        # make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "        # we assume d_v always equals d_k\n",
    "        self.d_k = d_model // h  # dimension of vector seen by each head\n",
    "        self.wq = nn.Linear(d_model, d_model, bias=False)  # Wq\n",
    "        self.wk = nn.Linear(d_model, d_model, bias=False)  # Wk\n",
    "        self.wv = nn.Linear(d_model, d_model, bias=False)  # Wv\n",
    "        self.wo = nn.Linear(d_model, d_model, bias=False)  # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask=None, dropout=None):\n",
    "        d_k = query.size(-1)\n",
    "        # calculate the attention scores by applying scaled dot-product attention\n",
    "        # query(batch, h, seq_len, d_k) * key_transpose(batch, h, d_k, seq_len) = (batch, h, seq_len, seq_len)\n",
    "        # in AIYUN paper, equation (1)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # Transpose the last 2 axis\n",
    "\n",
    "        if mask is not None:\n",
    "            # write a very low value (indicating -infinity) to the positions\n",
    "            # where mask == 0, this will tell softmax to replace those values\n",
    "            # with zero\n",
    "            attention_scores = attention_scores.masked_fill(mask==0, -1e9)\n",
    "            \n",
    "        # convert the attention scores to probability scores by softmax (# in AIYUN paper, equation (1))\n",
    "        attention_scores = attention_scores.softmax(dim=-1)  # (batch, h, seq_len, seq_len)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        \n",
    "        # Return a tuple containing the final matrix --> (batch, h, seq_len, d_k) & \n",
    "        # attention_scores --> (batch, h, seq_len, seq_len)\n",
    "        return torch.matmul(attention_scores, value), attention_scores\n",
    "        \n",
    "        \n",
    "               \n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        '''\n",
    "        Preapare the query, key and value matrices as in notes (Q',K',V')\n",
    "        The multiplication with Q',K',V' does not change the shape of incoming matrices\n",
    "        '''\n",
    "        query = self.wq(q) # (batch, seq_len, d_model) --> (batch, seq_len,d_model)\n",
    "        key = self.wk(k)   # (batch, seq_len, d_model) --> (batch, seq_len,d_model)\n",
    "        value = self.wv(v) # (batch, seq_len, d_model) --> (batch, seq_len,d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k)  broken down into smaller chunks\n",
    "        # (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)   2nd & 3rd dimension is transposed\n",
    "        # (batch, h, seq_len, d_k) makes more sense b'cz u have 'h' chunks of (seq_len,d_k) matrices\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, self.dropout)\n",
    "\n",
    "        # Change the shape of x to original {concatenate all attention heads}\n",
    "        #  (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1,2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        \n",
    "        # Final multiplication with W0 {AIYUN Paper page 5 top section}\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model) \n",
    "        return self.wo(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the residual Connection module ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the 'add' part in 'add & norm' module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \"\"\"This is the 'add' part in the 'add and norm' block.\"\"\"\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        x: input\n",
    "        sublayer: the previous layer , different layers of the transformer architecture (eg: multi-head\n",
    "        attention, feed-forward network, etc.)\n",
    "\n",
    "        Returns the skip or residual connection.\n",
    "        \"\"\"\n",
    "        # most implementations first do normalization and then pass x to the sublayer\n",
    "        # we will also do this way\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "        # however, the paper first passes x to the sublayer and then does the norm\n",
    "        # return x + self.dropout(self.norm(sublayer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the entire Encoder block  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, features: int, selfattn_block: MultiHeadAttentionBlock,\n",
    "                 feedforward_block: PositionWiseFeedForward, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.selfattn_block = selfattn_block\n",
    "        self.feedforward_block = feedforward_block\n",
    "        # store 2 residual connection layers\n",
    "        # we'l use one after self-attention layer and the other after feed-forward\n",
    "        # network as shown in figure 1 of the paper\n",
    "        self.res_con = nn.ModuleList([ResidualConnection(features, dropout)\n",
    "                                      for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        # we apply the source mask because we don't want the padding word to\n",
    "        # interact with other words\n",
    "        x = self.res_con[0](x, lambda x: self.selfattn_block(x,x,x,src_mask))\n",
    "        x = self.res_con[1](x, self.feedforward_block)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
